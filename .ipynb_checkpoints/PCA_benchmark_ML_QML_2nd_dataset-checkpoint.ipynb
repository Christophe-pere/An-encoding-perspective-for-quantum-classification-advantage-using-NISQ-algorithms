{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41d3fa6-0ae0-490d-a02b-0dbf81799488",
   "metadata": {},
   "source": [
    "# Quantum benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6011e42-1fab-4f9f-ab99-f4f08607517e",
   "metadata": {},
   "source": [
    "## I - Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4f6b4-240b-4b7f-abe4-4e74b3b8cc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd5492ce-4f08-4755-b4c2-efd698139101",
   "metadata": {},
   "source": [
    "## II - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90371200-0133-46ab-a205-d0f6b8858e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "#Import classical libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import functools\n",
    "\n",
    "from qiskit import BasicAer\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "from qiskit_machine_learning.kernels import QuantumKernel\n",
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "import logging\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates.embeddings import AngleEmbedding, AmplitudeEmbedding\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "from qiskit.algorithms.optimizers import COBYLA\n",
    "from qiskit.circuit.library import TwoLocal, ZZFeatureMap\n",
    "import qiskit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598c94c-fdcf-41cd-a4ea-e262932c7e0f",
   "metadata": {},
   "source": [
    "## III - Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e683f3b5-6021-4eb7-bc53-539a5cd79bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out CSV\n",
    "\n",
    "df = pd.read_csv('fraud_detection_bank_dataset.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf69c7f1-b10e-4b54-89ec-89dd5829ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import sweetviz as sv\n",
    "\n",
    "#EDA using Autoviz\n",
    "sweet_report = sv.analyze(df)\n",
    "\n",
    "#Saving results to HTML file\n",
    "sweet_report.show_html('sweet_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff48fa-d03b-435d-9343-67186c068b34",
   "metadata": {},
   "source": [
    "## IV - Modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe1bb7-d7eb-412b-a291-bbeb654766f5",
   "metadata": {},
   "source": [
    "### Classical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7f6037b-3b1c-4a2c-8a9e-df1d432cafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "df_labels = df['targets']\n",
    "df.drop(['targets'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "249ef3a0-7be3-4234-8be6-630db2cd267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, df_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd918a-57d8-443f-861d-77609bdb67a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1251e3e-c8ed-4e57-a790-1fa8003f8c31",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12b4f425-f8b1-40b4-8c3f-4502a9a01a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a48d4b60-5675-473e-88a1-77dcff67374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53c4b754-1ee5-42b6-a8ec-2d9f32b5762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f416ff2-f1e5-4f0e-b837-b0942f87a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['col_8', 'col_9', 'col_10', 'col_11', 'col_12', 'col_18', 'col_19','col_20', 'col_21', 'col_35', \n",
    "        'col_51', 'col_52', 'col_53', 'col_70','col_71','col_7', 'col_22', 'col_54', 'col_56']\n",
    "\n",
    "X_train = X_train.drop(cols, axis=1)\n",
    "X_test = X_test.drop(cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c0ac4-d233-443e-982d-e431f523495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = StandardScaler().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a948c1c2-cb55-49b2-bb4c-c2d2fdca527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(n_components=2)\n",
    "pca = PCA(n_components=2)\n",
    "pca = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58737fde-c826-4acf-8c94-cb8f4df345bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_train = pca.transform(X_train)\n",
    "df_pca_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3228f8e5-9afe-4127-9b40-c70fbd78c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_train = pd.DataFrame(data = df_pca_train\n",
    "             , columns = ['pc_1', 'pc_2'])\n",
    "\n",
    "df_pca_test = pd.DataFrame(data = df_pca_test\n",
    "             , columns = ['pc_1', 'pc_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb5a4c4a-13aa-4bcc-b388-f8641146ddca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc_1</th>\n",
       "      <th>pc_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-37252.963487</td>\n",
       "      <td>-134.646498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-36614.757077</td>\n",
       "      <td>-134.458136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-34974.747159</td>\n",
       "      <td>-133.918582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23534.372260</td>\n",
       "      <td>-129.967247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37028.564180</td>\n",
       "      <td>-134.657561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pc_1        pc_2\n",
       "0 -37252.963487 -134.646498\n",
       "1 -36614.757077 -134.458136\n",
       "2 -34974.747159 -133.918582\n",
       "3  23534.372260 -129.967247\n",
       "4 -37028.564180 -134.657561"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pca_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141a967-8c2f-472e-a934-5ad8ac0d9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(df_pca_train.iloc[:,0], df_pca_train.iloc[:,1], 'b+')\n",
    "plt.plot(df_pca_test.iloc[:,0], df_pca_test.iloc[:,1], 'g+')\n",
    "#plt.plot(df.index, feature_2, 'g+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4b4b4e7-4a73-4323-9424-a05d62cd8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca_1 = pd.DataFrame(feature_1)\n",
    "features_pca_2 = pd.DataFrame(feature_2)\n",
    "features_pca = features_pca_1.join(features_pca_2, lsuffix=\"_left\", rsuffix=\"_right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1de5099-55d4-4d2d-9ea4-eaf60c27b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_dim = len(df_pca_train.columns)\n",
    "n_dim = len(df_pca_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd626d5c-ff6a-443a-9700-10a93c24dc74",
   "metadata": {},
   "source": [
    "## Split train test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afaf0912-bdbf-4460-ae4c-43016d1f5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test\n",
    "\n",
    "sample_train, sample_test, label_train, label_test = train_test_split(\n",
    "     df_pca_train, y_train, test_size=0.2, random_state=22)\n",
    "\n",
    "# Normalize\n",
    "\n",
    "#std_scale = StandardScaler().fit(sample_train)\n",
    "#sample_train = std_scale.transform(sample_train)\n",
    "#sample_test = std_scale.transform(sample_test)\n",
    "\n",
    "# Scale for better fit within the feature map\n",
    "\n",
    "#samples = np.append(sample_train, sample_test, axis=0)\n",
    "#minmax_scale = MinMaxScaler((-1, 1)).fit(samples)\n",
    "#sample_train = minmax_scale.transform(sample_train)\n",
    "#sample_test = minmax_scale.transform(sample_test)\n",
    "\n",
    "# Select a sample for a better control of the research and wall time\n",
    "\n",
    "train_size = 800#160\n",
    "sample_train = sample_train[:train_size]\n",
    "label_train = label_train[:train_size]\n",
    "\n",
    "test_size = 200 #40\n",
    "sample_test = sample_test[:test_size]\n",
    "label_test = label_test[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d815edbe-c5e7-42d6-9489-39ebb940cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic parameters for hybrid model\n",
    "\n",
    "seed = 8500\n",
    "feature_dim = n_dim\n",
    "num_reps = 2\n",
    "num_shots =256 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17bc9af-4a1c-453d-be99-d54b9e045628",
   "metadata": {},
   "source": [
    "## Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2ce7078-9255-4913-b6aa-7f05a0e077c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callable kernel classification test score: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Define feature_map\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=feature_dim, reps=num_reps)\n",
    "\n",
    "# Define the backend\n",
    "backend = QuantumInstance(\n",
    "    BasicAer.get_backend(\"qasm_simulator\"), shots=num_shots, seed_simulator=seed, seed_transpiler=seed\n",
    ")\n",
    "\n",
    "# Define the kernel\n",
    "\n",
    "kernel = QuantumKernel(feature_map=feature_map, quantum_instance=backend)\n",
    "\n",
    "# Model run\n",
    "svc = SVC(kernel=kernel.evaluate)\n",
    "#svc.fit(sample_train, label_train)\n",
    "#score = svc.score(sample_test, label_test)\n",
    "\n",
    "#print(f\"Callable kernel classification test score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2abf51e2-5f63-4356-8aa6-381d826279b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_models = []\n",
    "_models.append(('qsvc',svc))\n",
    "_metrics = ['precision', 'recall', 'f1', 'accuracy',  'matthews_corrcoef','balanced_accuracy']\n",
    "for metric in _metrics:\n",
    "    df_results= pd.concat([df_results, evaluate_ml_model(_models, sample_train, label_train, n_fold=10, metric=metric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b357230f-6d30-41ac-b3ab-6767ab058cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision mean (%)</th>\n",
       "      <th>precision std (%)</th>\n",
       "      <th>recall mean (%)</th>\n",
       "      <th>recall std (%)</th>\n",
       "      <th>f1 mean (%)</th>\n",
       "      <th>f1 std (%)</th>\n",
       "      <th>accuracy mean (%)</th>\n",
       "      <th>accuracy std (%)</th>\n",
       "      <th>balanced_accuracy mean (%)</th>\n",
       "      <th>balanced_accuracy std (%)</th>\n",
       "      <th>...</th>\n",
       "      <th>recall mean (%)</th>\n",
       "      <th>recall std (%)</th>\n",
       "      <th>f1 mean (%)</th>\n",
       "      <th>f1 std (%)</th>\n",
       "      <th>accuracy mean (%)</th>\n",
       "      <th>accuracy std (%)</th>\n",
       "      <th>matthews_corrcoef mean (%)</th>\n",
       "      <th>matthews_corrcoef std (%)</th>\n",
       "      <th>balanced_accuracy mean (%)</th>\n",
       "      <th>balanced_accuracy std (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.63</td>\n",
       "      <td>3.26</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>37.56</td>\n",
       "      <td>10.92</td>\n",
       "      <td>24.59</td>\n",
       "      <td>8.44</td>\n",
       "      <td>29.42</td>\n",
       "      <td>9.12</td>\n",
       "      <td>70.75</td>\n",
       "      <td>2.86</td>\n",
       "      <td>55.42</td>\n",
       "      <td>3.92</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CART</th>\n",
       "      <td>20.17</td>\n",
       "      <td>19.01</td>\n",
       "      <td>7.01</td>\n",
       "      <td>6.95</td>\n",
       "      <td>10.23</td>\n",
       "      <td>10.09</td>\n",
       "      <td>70.50</td>\n",
       "      <td>2.45</td>\n",
       "      <td>49.27</td>\n",
       "      <td>3.88</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB</th>\n",
       "      <td>27.37</td>\n",
       "      <td>4.21</td>\n",
       "      <td>94.51</td>\n",
       "      <td>6.48</td>\n",
       "      <td>42.35</td>\n",
       "      <td>5.66</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3.58</td>\n",
       "      <td>54.52</td>\n",
       "      <td>2.66</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.63</td>\n",
       "      <td>3.26</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qsvc</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.63</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      precision mean (%)  precision std (%)  recall mean (%)  recall std (%)  \\\n",
       "LR                  0.00               0.00             0.00            0.00   \n",
       "KNN                37.56              10.92            24.59            8.44   \n",
       "CART               20.17              19.01             7.01            6.95   \n",
       "NB                 27.37               4.21            94.51            6.48   \n",
       "SVM                 0.00               0.00             0.00            0.00   \n",
       "qsvc                 NaN                NaN              NaN             NaN   \n",
       "\n",
       "      f1 mean (%)  f1 std (%)  accuracy mean (%)  accuracy std (%)  \\\n",
       "LR           0.00        0.00              74.63              3.26   \n",
       "KNN         29.42        9.12              70.75              2.86   \n",
       "CART        10.23       10.09              70.50              2.45   \n",
       "NB          42.35        5.66              35.00              3.58   \n",
       "SVM          0.00        0.00              74.63              3.26   \n",
       "qsvc          NaN         NaN                NaN               NaN   \n",
       "\n",
       "      balanced_accuracy mean (%)  balanced_accuracy std (%)  ...  \\\n",
       "LR                         50.00                       0.00  ...   \n",
       "KNN                        55.42                       3.92  ...   \n",
       "CART                       49.27                       3.88  ...   \n",
       "NB                         54.52                       2.66  ...   \n",
       "SVM                        50.00                       0.00  ...   \n",
       "qsvc                         NaN                        NaN  ...   \n",
       "\n",
       "      recall mean (%)  recall std (%)  f1 mean (%)  f1 std (%)  \\\n",
       "LR                NaN             NaN          NaN         NaN   \n",
       "KNN               NaN             NaN          NaN         NaN   \n",
       "CART              NaN             NaN          NaN         NaN   \n",
       "NB                NaN             NaN          NaN         NaN   \n",
       "SVM               NaN             NaN          NaN         NaN   \n",
       "qsvc              0.0             0.0          0.0         0.0   \n",
       "\n",
       "      accuracy mean (%)  accuracy std (%)  matthews_corrcoef mean (%)  \\\n",
       "LR                  NaN               NaN                         NaN   \n",
       "KNN                 NaN               NaN                         NaN   \n",
       "CART                NaN               NaN                         NaN   \n",
       "NB                  NaN               NaN                         NaN   \n",
       "SVM                 NaN               NaN                         NaN   \n",
       "qsvc              74.63              3.26                         0.0   \n",
       "\n",
       "      matthews_corrcoef std (%)  balanced_accuracy mean (%)  \\\n",
       "LR                          NaN                         NaN   \n",
       "KNN                         NaN                         NaN   \n",
       "CART                        NaN                         NaN   \n",
       "NB                          NaN                         NaN   \n",
       "SVM                         NaN                         NaN   \n",
       "qsvc                        0.0                        50.0   \n",
       "\n",
       "      balanced_accuracy std (%)  \n",
       "LR                          NaN  \n",
       "KNN                         NaN  \n",
       "CART                        NaN  \n",
       "NB                          NaN  \n",
       "SVM                         NaN  \n",
       "qsvc                        0.0  \n",
       "\n",
       "[6 rows x 26 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1d4e4-599e-47ea-b53c-dc200ee4dec0",
   "metadata": {},
   "source": [
    "## Classical Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d93f1b24-b419-4eb0-abb5-fd297fcec137",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(max_iter=1000)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a92689bc-07b9-4bdd-b25c-0b0bae45a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5bd332f9-cc4e-43eb-9cd9-0990b709ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ml_model(models, X, y, n_fold=10, metric='precision'):\n",
    "    \n",
    "    _df = pd.DataFrame()\n",
    "    #results = []\n",
    "    names = []\n",
    "    #scoring = 'accuracy'\n",
    "    for name, model in models:\n",
    "        kfold = KFold(n_splits=n_fold)\n",
    "        cv_results = cross_val_score(model, X, y, cv=kfold, scoring=metric)\n",
    "        #results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        #print(msg)\n",
    "        _df =  pd.concat([_df, pd.DataFrame([round(100*cv_results.mean(), 2) , round(100*cv_results.std(), 2) ]).T])\n",
    "    _df.index = names\n",
    "    _df.columns=[metric+' mean (%)', metric+' std (%)']\n",
    "    return _df \n",
    "             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89520867-4e75-4e63-ae1e-10842771f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "_metrics = ['precision', 'recall', 'f1', 'accuracy', 'balanced_accuracy', 'matthews_corrcoef']\n",
    "for metric in _metrics:\n",
    "    df_results= pd.concat([df_results, evaluate_ml_model(models, sample_train, label_train, n_fold=10, metric=metric)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15d5c6b3-7172-430b-b803-f60126a12cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results= pd.concat([df_results, evaluate_ml_model(models, sample_train, label_train, n_fold=10, metric='accuracy')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8561d92d-f3ee-4045-b6e5-98097a82f364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision mean (%)</th>\n",
       "      <th>precision std (%)</th>\n",
       "      <th>recall mean (%)</th>\n",
       "      <th>recall std (%)</th>\n",
       "      <th>f1 mean (%)</th>\n",
       "      <th>f1 std (%)</th>\n",
       "      <th>accuracy mean (%)</th>\n",
       "      <th>accuracy std (%)</th>\n",
       "      <th>balanced_accuracy mean (%)</th>\n",
       "      <th>balanced_accuracy std (%)</th>\n",
       "      <th>matthews_corrcoef mean (%)</th>\n",
       "      <th>matthews_corrcoef std (%)</th>\n",
       "      <th>accuracy mean (%)</th>\n",
       "      <th>accuracy std (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.63</td>\n",
       "      <td>3.26</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.63</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>37.56</td>\n",
       "      <td>10.92</td>\n",
       "      <td>24.59</td>\n",
       "      <td>8.44</td>\n",
       "      <td>29.42</td>\n",
       "      <td>9.12</td>\n",
       "      <td>70.75</td>\n",
       "      <td>2.86</td>\n",
       "      <td>55.42</td>\n",
       "      <td>3.92</td>\n",
       "      <td>12.62</td>\n",
       "      <td>9.11</td>\n",
       "      <td>70.75</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CART</th>\n",
       "      <td>20.17</td>\n",
       "      <td>19.01</td>\n",
       "      <td>7.01</td>\n",
       "      <td>6.95</td>\n",
       "      <td>10.23</td>\n",
       "      <td>10.09</td>\n",
       "      <td>70.50</td>\n",
       "      <td>2.45</td>\n",
       "      <td>49.27</td>\n",
       "      <td>3.88</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>12.46</td>\n",
       "      <td>70.62</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB</th>\n",
       "      <td>27.37</td>\n",
       "      <td>4.21</td>\n",
       "      <td>94.51</td>\n",
       "      <td>6.48</td>\n",
       "      <td>42.35</td>\n",
       "      <td>5.66</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3.58</td>\n",
       "      <td>54.52</td>\n",
       "      <td>2.66</td>\n",
       "      <td>12.92</td>\n",
       "      <td>7.13</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.63</td>\n",
       "      <td>3.26</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.63</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      precision mean (%)  precision std (%)  recall mean (%)  recall std (%)  \\\n",
       "LR                  0.00               0.00             0.00            0.00   \n",
       "KNN                37.56              10.92            24.59            8.44   \n",
       "CART               20.17              19.01             7.01            6.95   \n",
       "NB                 27.37               4.21            94.51            6.48   \n",
       "SVM                 0.00               0.00             0.00            0.00   \n",
       "\n",
       "      f1 mean (%)  f1 std (%)  accuracy mean (%)  accuracy std (%)  \\\n",
       "LR           0.00        0.00              74.63              3.26   \n",
       "KNN         29.42        9.12              70.75              2.86   \n",
       "CART        10.23       10.09              70.50              2.45   \n",
       "NB          42.35        5.66              35.00              3.58   \n",
       "SVM          0.00        0.00              74.63              3.26   \n",
       "\n",
       "      balanced_accuracy mean (%)  balanced_accuracy std (%)  \\\n",
       "LR                         50.00                       0.00   \n",
       "KNN                        55.42                       3.92   \n",
       "CART                       49.27                       3.88   \n",
       "NB                         54.52                       2.66   \n",
       "SVM                        50.00                       0.00   \n",
       "\n",
       "      matthews_corrcoef mean (%)  matthews_corrcoef std (%)  \\\n",
       "LR                          0.00                       0.00   \n",
       "KNN                        12.62                       9.11   \n",
       "CART                       -1.55                      12.46   \n",
       "NB                         12.92                       7.13   \n",
       "SVM                         0.00                       0.00   \n",
       "\n",
       "      accuracy mean (%)  accuracy std (%)  \n",
       "LR                74.63              3.26  \n",
       "KNN               70.75              2.86  \n",
       "CART              70.62              2.32  \n",
       "NB                35.00              3.58  \n",
       "SVM               74.63              3.26  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70b837-4e39-4b65-83ca-8e82fc83efa1",
   "metadata": {},
   "source": [
    "## Pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff1d5656-f2f2-4d96-b210-a75a161cdb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07ec49f5-c401-4c36-aae3-57aad3eaa0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angle Encoding\n",
    "\n",
    "num_qubits = n_dim\n",
    "\n",
    "dev = qml.device('default.qubit', wires = num_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(parameters, data):\n",
    "    for i in range(num_qubits):\n",
    "        qml.Hadamard(wires = i)\n",
    "    \n",
    "    AngleEmbedding(features = data, wires = range(num_qubits), rotation = 'Y')\n",
    "    \n",
    "    qml.StronglyEntanglingLayers(weights = parameters, wires = range(num_qubits))\n",
    "    \n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffcf8fdb-37a0-4b64-b73c-76be553995e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "#print(weights_init, bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49568595-37ec-4749-a230-dc4c6dae2500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.09425205, requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit(weights_init, sample_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4febb380-0955-4f05-a872-326cc4e3a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c16a1f84-63e5-4fbb-a647-cba6db4c6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07412a53-c93b-4c73-bd0b-e7e144454435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bf0b58e-38f3-4c45-a4d2-3d69af787ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21ff4aa6-c15b-4a59-a718-67d8da2402dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [tensor(-0.99999959, requires_grad=True), tensor(0.10477704, requires_grad=True)], Y = -1\n",
      "X = [tensor(-0.99999897, requires_grad=True), tensor(-0.61489866, requires_grad=True)], Y = -1\n",
      "X = [tensor(-0.99627189, requires_grad=True), tensor(-0.88545579, requires_grad=True)], Y = -1\n",
      "X = [tensor(-0.99994599, requires_grad=True), tensor(-0.10680072, requires_grad=True)], Y = -1\n",
      "X = [tensor(-0.99230716, requires_grad=True), tensor(-0.14662807, requires_grad=True)], Y = -1\n"
     ]
    }
   ],
   "source": [
    "Y = np.array(label_train * 2 - np.ones(len(label_train)),requires_grad=True)  # shift label from {0, 1} to {-1, 1}\n",
    "X = np.array(sample_train, requires_grad=True)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"X = {}, Y = {: d}\".format(list(X[i]), int(Y[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "074f578a-1984-4814-9070-7b20063c8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = AdamOptimizer(stepsize=0.1, beta1=0.9, beta2=0.99, eps=1e-08)\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8b4bf5c-322d-43e2-ac30-3bf83e3e7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d31d6a7-74f4-4e63-b1cb-b92bfb610a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best:\n",
      "Iter:     1 | Cost: 1.9191211 | f1: 0.1154299 \n",
      "New Best:\n",
      "Iter:     2 | Cost: 1.8256290 | f1: 0.2608696 \n",
      "Iter:     3 | Cost: 1.2961935 | f1: 0.0000000 \n",
      "Iter:     4 | Cost: 0.8427779 | f1: 0.0000000 \n",
      "Iter:     5 | Cost: 0.7063916 | f1: 0.0000000 \n",
      "Iter:     6 | Cost: 0.6819693 | f1: 0.0000000 \n",
      "Iter:     7 | Cost: 0.5885872 | f1: 0.0000000 \n",
      "Iter:     8 | Cost: 0.4792729 | f1: 0.0000000 \n",
      "New Best:\n",
      "Iter:     9 | Cost: 0.5767016 | f1: 0.8202020 \n",
      "Iter:    10 | Cost: 0.7209637 | f1: 0.6558966 \n",
      "Iter:    11 | Cost: 0.6950332 | f1: 0.6710744 \n",
      "New Best:\n",
      "Iter:    12 | Cost: 0.5077988 | f1: 0.8750000 \n",
      "Iter:    13 | Cost: 0.4284217 | f1: 0.6753247 \n",
      "Iter:    14 | Cost: 0.4392933 | f1: 0.0000000 \n",
      "Iter:    15 | Cost: 0.4780805 | f1: 0.0000000 \n",
      "Iter:    16 | Cost: 0.5272714 | f1: 0.0000000 \n",
      "Iter:    17 | Cost: 0.5518219 | f1: 0.0000000 \n",
      "Iter:    18 | Cost: 0.4958795 | f1: 0.0000000 \n",
      "Iter:    19 | Cost: 0.5037093 | f1: 0.0000000 \n",
      "Iter:    20 | Cost: 0.5706935 | f1: 0.5902778 \n",
      "Iter:    21 | Cost: 0.4720448 | f1: 0.0000000 \n",
      "Iter:    22 | Cost: 0.4664287 | f1: 0.0000000 \n",
      "Iter:    23 | Cost: 0.4803494 | f1: 0.0000000 \n",
      "Iter:    24 | Cost: 0.4820225 | f1: 0.0000000 \n",
      "Iter:    25 | Cost: 0.4660904 | f1: 0.0000000 \n",
      "Iter:    26 | Cost: 0.4104238 | f1: 0.3292181 \n",
      "Iter:    27 | Cost: 0.4099678 | f1: 0.6818182 \n",
      "New Best:\n",
      "Iter:    28 | Cost: 0.4380423 | f1: 0.9371728 \n",
      "New Best:\n",
      "Iter:    29 | Cost: 0.4679770 | f1: 0.9376443 \n",
      "New Best:\n",
      "Iter:    30 | Cost: 0.4615990 | f1: 0.9441860 \n",
      "Iter:    31 | Cost: 0.4159314 | f1: 0.7659574 \n",
      "Iter:    32 | Cost: 0.4388377 | f1: 0.0000000 \n",
      "Iter:    33 | Cost: 0.5022183 | f1: 0.0000000 \n",
      "Iter:    34 | Cost: 0.4581624 | f1: 0.0000000 \n",
      "Iter:    35 | Cost: 0.3987200 | f1: 0.4736842 \n",
      "Iter:    36 | Cost: 0.3861415 | f1: 0.7584098 \n",
      "Iter:    37 | Cost: 0.3844834 | f1: 0.7584098 \n",
      "Iter:    38 | Cost: 0.3877395 | f1: 0.7659574 \n",
      "Iter:    39 | Cost: 0.3924685 | f1: 0.7893175 \n",
      "Iter:    40 | Cost: 0.3964198 | f1: 0.7893175 \n",
      "Iter:    41 | Cost: 0.4033627 | f1: 0.4736842 \n",
      "Iter:    42 | Cost: 0.4065538 | f1: 0.3694779 \n",
      "Iter:    43 | Cost: 0.3988448 | f1: 0.4719101 \n",
      "Iter:    44 | Cost: 0.3926733 | f1: 0.5217391 \n",
      "Iter:    45 | Cost: 0.3878782 | f1: 0.5531915 \n",
      "Iter:    46 | Cost: 0.3814918 | f1: 0.6308725 \n",
      "Iter:    47 | Cost: 0.3825783 | f1: 0.5633803 \n",
      "Iter:    48 | Cost: 0.3801948 | f1: 0.6075085 \n",
      "Iter:    49 | Cost: 0.3771705 | f1: 0.7928994 \n",
      "Iter:    50 | Cost: 0.3839273 | f1: 0.8972973 \n",
      "Iter:    51 | Cost: 0.3865296 | f1: 0.9061662 \n",
      "Iter:    52 | Cost: 0.3899879 | f1: 0.9263158 \n",
      "New Best:\n",
      "Iter:    53 | Cost: 0.4094857 | f1: 0.9575472 \n",
      "Iter:    54 | Cost: 0.4113702 | f1: 0.9419954 \n",
      "Iter:    55 | Cost: 0.4335571 | f1: 0.8601695 \n",
      "Iter:    56 | Cost: 0.4720271 | f1: 0.7960784 \n",
      "Iter:    57 | Cost: 0.5310739 | f1: 0.7355072 \n",
      "Iter:    58 | Cost: 0.4951660 | f1: 0.7703985 \n",
      "Iter:    59 | Cost: 0.4947683 | f1: 0.7703985 \n",
      "Iter:    60 | Cost: 0.4516461 | f1: 0.8336756 \n",
      "New Best:\n",
      "Iter:    61 | Cost: 0.4030995 | f1: 0.9736211 \n",
      "Iter:    62 | Cost: 0.3784301 | f1: 0.7928994 \n",
      "Iter:    63 | Cost: 0.4001062 | f1: 0.3481781 \n",
      "Iter:    64 | Cost: 0.4854334 | f1: 0.0000000 \n",
      "Iter:    65 | Cost: 0.6276975 | f1: 0.0000000 \n",
      "Iter:    66 | Cost: 0.7017121 | f1: 0.0000000 \n",
      "Iter:    67 | Cost: 0.7613707 | f1: 0.0000000 \n",
      "Iter:    68 | Cost: 0.6921267 | f1: 0.0000000 \n",
      "Iter:    69 | Cost: 0.5216963 | f1: 0.0000000 \n",
      "Iter:    70 | Cost: 0.4129386 | f1: 0.2270742 \n",
      "Iter:    71 | Cost: 0.4094952 | f1: 0.8441926 \n",
      "Iter:    72 | Cost: 0.4791514 | f1: 0.8806941 \n",
      "Iter:    73 | Cost: 0.5618941 | f1: 0.7532468 \n",
      "Iter:    74 | Cost: 0.5733669 | f1: 0.7302158 \n",
      "Iter:    75 | Cost: 0.5477876 | f1: 0.7449541 \n",
      "Iter:    76 | Cost: 0.5105980 | f1: 0.7762906 \n",
      "Iter:    77 | Cost: 0.4683334 | f1: 0.8601695 \n",
      "Iter:    78 | Cost: 0.4305677 | f1: 0.9263158 \n",
      "Iter:    79 | Cost: 0.4293327 | f1: 0.9148936 \n",
      "Iter:    80 | Cost: 0.4186231 | f1: 0.7636364 \n",
      "Iter:    81 | Cost: 0.4168479 | f1: 0.5018450 \n",
      "Iter:    82 | Cost: 0.4118932 | f1: 0.5236364 \n",
      "Iter:    83 | Cost: 0.4166788 | f1: 0.3292181 \n",
      "Iter:    84 | Cost: 0.4479047 | f1: 0.0000000 \n",
      "Iter:    85 | Cost: 0.4924810 | f1: 0.0000000 \n",
      "Iter:    86 | Cost: 0.4839052 | f1: 0.0000000 \n",
      "Iter:    87 | Cost: 0.4308926 | f1: 0.0000000 \n",
      "Iter:    88 | Cost: 0.4055005 | f1: 0.7747748 \n",
      "Iter:    89 | Cost: 0.4734705 | f1: 0.8787879 \n",
      "Iter:    90 | Cost: 0.6209221 | f1: 0.7060870 \n",
      "Iter:    91 | Cost: 0.5534690 | f1: 0.7560521 \n",
      "Iter:    92 | Cost: 0.4536235 | f1: 0.9398148 \n",
      "Iter:    93 | Cost: 0.4052562 | f1: 0.7636364 \n",
      "Iter:    94 | Cost: 0.4084065 | f1: 0.3680000 \n",
      "Iter:    95 | Cost: 0.4302925 | f1: 0.0000000 \n",
      "Iter:    96 | Cost: 0.4505447 | f1: 0.0000000 \n",
      "Iter:    97 | Cost: 0.4312088 | f1: 0.0000000 \n",
      "Iter:    98 | Cost: 0.4248091 | f1: 0.0845070 \n",
      "Iter:    99 | Cost: 0.4009376 | f1: 0.5684211 \n",
      "Iter:   100 | Cost: 0.4189995 | f1: 0.8972973 \n",
      "New Best:\n",
      "Iter:   101 | Cost: 0.4384257 | f1: 0.9876543 \n",
      "Iter:   102 | Cost: 0.4328275 | f1: 0.9591837 \n",
      "Iter:   103 | Cost: 0.4185908 | f1: 0.8913043 \n",
      "Iter:   104 | Cost: 0.4044329 | f1: 0.7636364 \n",
      "Iter:   105 | Cost: 0.3990385 | f1: 0.5684211 \n",
      "Iter:   106 | Cost: 0.3978280 | f1: 0.5109489 \n",
      "Iter:   107 | Cost: 0.3931948 | f1: 0.5531915 \n",
      "Iter:   108 | Cost: 0.3881698 | f1: 0.5979381 \n",
      "Iter:   109 | Cost: 0.3824851 | f1: 0.7560976 \n",
      "Iter:   110 | Cost: 0.3837933 | f1: 0.8852459 \n",
      "Iter:   111 | Cost: 0.3950680 | f1: 0.9825436 \n",
      "Iter:   112 | Cost: 0.4182327 | f1: 0.9082774 \n",
      "Iter:   113 | Cost: 0.4192423 | f1: 0.9002217 \n",
      "Iter:   114 | Cost: 0.4074082 | f1: 0.9354839 \n",
      "New Best:\n",
      "Iter:   115 | Cost: 0.3944948 | f1: 0.9950980 \n",
      "Iter:   116 | Cost: 0.3834597 | f1: 0.9457364 \n",
      "Iter:   117 | Cost: 0.3765755 | f1: 0.8571429 \n",
      "Iter:   118 | Cost: 0.3753192 | f1: 0.8242075 \n",
      "Iter:   119 | Cost: 0.3747718 | f1: 0.7368421 \n",
      "Iter:   120 | Cost: 0.3800933 | f1: 0.5754386 \n",
      "Iter:   121 | Cost: 0.3879486 | f1: 0.4736842 \n",
      "Iter:   122 | Cost: 0.3948578 | f1: 0.3952569 \n",
      "Iter:   123 | Cost: 0.3960975 | f1: 0.3760000 \n",
      "Iter:   124 | Cost: 0.3945275 | f1: 0.3952569 \n",
      "Iter:   125 | Cost: 0.3904174 | f1: 0.4503817 \n",
      "Iter:   126 | Cost: 0.3858901 | f1: 0.5018450 \n",
      "Iter:   127 | Cost: 0.3779796 | f1: 0.6666667 \n",
      "Iter:   128 | Cost: 0.3767752 | f1: 0.7636364 \n",
      "Iter:   129 | Cost: 0.3782840 | f1: 0.8208092 \n",
      "Iter:   130 | Cost: 0.3831761 | f1: 0.8913043 \n",
      "Iter:   131 | Cost: 0.3911368 | f1: 0.9565217 \n",
      "Iter:   132 | Cost: 0.4120210 | f1: 0.9485981 \n",
      "Iter:   133 | Cost: 0.4081613 | f1: 0.9598109 \n",
      "Iter:   134 | Cost: 0.3877270 | f1: 0.9565217 \n",
      "Iter:   135 | Cost: 0.3768060 | f1: 0.8603352 \n",
      "Iter:   136 | Cost: 0.3751491 | f1: 0.8342857 \n",
      "Iter:   137 | Cost: 0.3750750 | f1: 0.7928994 \n",
      "Iter:   138 | Cost: 0.3779705 | f1: 0.6666667 \n",
      "Iter:   139 | Cost: 0.3851088 | f1: 0.5376344 \n",
      "Iter:   140 | Cost: 0.3837658 | f1: 0.5833333 \n",
      "Iter:   141 | Cost: 0.3896074 | f1: 0.8913043 \n",
      "Iter:   142 | Cost: 0.4552518 | f1: 0.8405797 \n",
      "Iter:   143 | Cost: 0.5474949 | f1: 0.7262970 \n",
      "Iter:   144 | Cost: 0.5674017 | f1: 0.7185841 \n",
      "Iter:   145 | Cost: 0.4995246 | f1: 0.7674858 \n",
      "Iter:   146 | Cost: 0.4154747 | f1: 0.9185520 \n",
      "Iter:   147 | Cost: 0.3755856 | f1: 0.8309456 \n",
      "Iter:   148 | Cost: 0.3866095 | f1: 0.4888889 \n",
      "Iter:   149 | Cost: 0.4099524 | f1: 0.2192982 \n",
      "Iter:   150 | Cost: 0.4297184 | f1: 0.0000000 \n",
      "Iter:   151 | Cost: 0.4234751 | f1: 0.0666667 \n",
      "Iter:   152 | Cost: 0.3995155 | f1: 0.3428571 \n",
      "Iter:   153 | Cost: 0.3824259 | f1: 0.5531915 \n",
      "Iter:   154 | Cost: 0.3755157 | f1: 0.7784431 \n",
      "Iter:   155 | Cost: 0.3876870 | f1: 0.9565217 \n",
      "Iter:   156 | Cost: 0.4098940 | f1: 0.9290618 \n",
      "Iter:   157 | Cost: 0.4268407 | f1: 0.8768898 \n",
      "Iter:   158 | Cost: 0.4371084 | f1: 0.8547368 \n",
      "Iter:   159 | Cost: 0.4234318 | f1: 0.8942731 \n",
      "Iter:   160 | Cost: 0.4168424 | f1: 0.9206349 \n",
      "Iter:   161 | Cost: 0.4104177 | f1: 0.9419954 \n",
      "Iter:   162 | Cost: 0.3981603 | f1: 0.9901478 \n",
      "Iter:   163 | Cost: 0.3882656 | f1: 0.9375000 \n",
      "Iter:   164 | Cost: 0.3812521 | f1: 0.8635097 \n",
      "Iter:   165 | Cost: 0.3788717 | f1: 0.6964856 \n",
      "Iter:   166 | Cost: 0.3824639 | f1: 0.5783972 \n",
      "Iter:   167 | Cost: 0.3957164 | f1: 0.3937008 \n",
      "Iter:   168 | Cost: 0.4080677 | f1: 0.2413793 \n",
      "Iter:   169 | Cost: 0.4078240 | f1: 0.2413793 \n",
      "Iter:   170 | Cost: 0.3994509 | f1: 0.3481781 \n",
      "Iter:   171 | Cost: 0.3891458 | f1: 0.4719101 \n",
      "Iter:   172 | Cost: 0.3781763 | f1: 0.6169492 \n",
      "Iter:   173 | Cost: 0.3744820 | f1: 0.7598784 \n",
      "Iter:   174 | Cost: 0.3850767 | f1: 0.9457364 \n",
      "Iter:   175 | Cost: 0.4113416 | f1: 0.9354839 \n",
      "Iter:   176 | Cost: 0.4112066 | f1: 0.9354839 \n",
      "Iter:   177 | Cost: 0.3958583 | f1: 0.9901478 \n",
      "Iter:   178 | Cost: 0.3860605 | f1: 0.9511568 \n",
      "Iter:   179 | Cost: 0.3966389 | f1: 0.9950980 \n",
      "Iter:   180 | Cost: 0.3973922 | f1: 0.9902439 \n",
      "Iter:   181 | Cost: 0.4050845 | f1: 0.9552941 \n",
      "Iter:   182 | Cost: 0.3862605 | f1: 0.9565217 \n",
      "Iter:   183 | Cost: 0.3749006 | f1: 0.8507042 \n",
      "Iter:   184 | Cost: 0.3726467 | f1: 0.7820896 \n",
      "Iter:   185 | Cost: 0.3731855 | f1: 0.7289720 \n",
      "Iter:   186 | Cost: 0.3739370 | f1: 0.6923077 \n",
      "Iter:   187 | Cost: 0.3747501 | f1: 0.6838710 \n",
      "Iter:   188 | Cost: 0.3752169 | f1: 0.6796117 \n",
      "Iter:   189 | Cost: 0.3757722 | f1: 0.6666667 \n",
      "Iter:   190 | Cost: 0.3769164 | f1: 0.6490066 \n",
      "Iter:   191 | Cost: 0.3824220 | f1: 0.5531915 \n",
      "Iter:   192 | Cost: 0.3794234 | f1: 0.5979381 \n",
      "Iter:   193 | Cost: 0.3759929 | f1: 0.6796117 \n",
      "Iter:   194 | Cost: 0.3740330 | f1: 0.7560976 \n",
      "Iter:   195 | Cost: 0.3736469 | f1: 0.7560976 \n",
      "Iter:   196 | Cost: 0.3732582 | f1: 0.7636364 \n",
      "Iter:   197 | Cost: 0.3736793 | f1: 0.8242075 \n",
      "Iter:   198 | Cost: 0.3755266 | f1: 0.8791209 \n",
      "Iter:   199 | Cost: 0.3758412 | f1: 0.8852459 \n",
      "Iter:   200 | Cost: 0.3784269 | f1: 0.9032258 \n",
      "Iter:   201 | Cost: 0.3801840 | f1: 0.9234828 \n",
      "Iter:   202 | Cost: 0.3825292 | f1: 0.9457364 \n",
      "Iter:   203 | Cost: 0.3803769 | f1: 0.9234828 \n",
      "Iter:   204 | Cost: 0.3779962 | f1: 0.8972973 \n",
      "Iter:   205 | Cost: 0.3768978 | f1: 0.8882834 \n",
      "Iter:   206 | Cost: 0.3742642 | f1: 0.8441926 \n",
      "Iter:   207 | Cost: 0.3732329 | f1: 0.8104956 \n",
      "Iter:   208 | Cost: 0.3729171 | f1: 0.7484663 \n",
      "Iter:   209 | Cost: 0.3754104 | f1: 0.6490066 \n",
      "Iter:   210 | Cost: 0.3749001 | f1: 0.6666667 \n",
      "Iter:   211 | Cost: 0.3730375 | f1: 0.7006369 \n",
      "Iter:   212 | Cost: 0.3722700 | f1: 0.7560976 \n",
      "Iter:   213 | Cost: 0.3724274 | f1: 0.7407407 \n",
      "Iter:   214 | Cost: 0.3725685 | f1: 0.7368421 \n",
      "Iter:   215 | Cost: 0.3752594 | f1: 0.6490066 \n",
      "Iter:   216 | Cost: 0.3769198 | f1: 0.6075085 \n",
      "Iter:   217 | Cost: 0.3780359 | f1: 0.5833333 \n",
      "Iter:   218 | Cost: 0.3759615 | f1: 0.6216216 \n",
      "Iter:   219 | Cost: 0.3732894 | f1: 0.6964856 \n",
      "Iter:   220 | Cost: 0.3772896 | f1: 0.5979381 \n",
      "Iter:   221 | Cost: 0.3767134 | f1: 0.6122449 \n",
      "Iter:   222 | Cost: 0.3763885 | f1: 0.6169492 \n",
      "Iter:   223 | Cost: 0.3753886 | f1: 0.6490066 \n",
      "Iter:   224 | Cost: 0.3738830 | f1: 0.6838710 \n",
      "Iter:   225 | Cost: 0.3755060 | f1: 0.6400000 \n",
      "Iter:   226 | Cost: 0.3795960 | f1: 0.5531915 \n",
      "Iter:   227 | Cost: 0.3818431 | f1: 0.5376344 \n",
      "Iter:   228 | Cost: 0.3933141 | f1: 0.4000000 \n",
      "Iter:   229 | Cost: 0.3872530 | f1: 0.4719101 \n",
      "Iter:   230 | Cost: 0.3902792 | f1: 0.4247104 \n",
      "Iter:   231 | Cost: 0.3967128 | f1: 0.3694779 \n",
      "Iter:   232 | Cost: 0.4021370 | f1: 0.3292181 \n",
      "Iter:   233 | Cost: 0.4143826 | f1: 0.1628959 \n",
      "Iter:   234 | Cost: 0.4196327 | f1: 0.1290323 \n",
      "Iter:   235 | Cost: 0.4175759 | f1: 0.1545455 \n",
      "Iter:   236 | Cost: 0.3976669 | f1: 0.3629032 \n",
      "Iter:   237 | Cost: 0.3783251 | f1: 0.5882353 \n",
      "Iter:   238 | Cost: 0.3727638 | f1: 0.7928994 \n",
      "Iter:   239 | Cost: 0.3780823 | f1: 0.8972973 \n",
      "Iter:   240 | Cost: 0.3837660 | f1: 0.9484536 \n",
      "Iter:   241 | Cost: 0.3836928 | f1: 0.9234828 \n",
      "Iter:   242 | Cost: 0.3800394 | f1: 0.8474576 \n",
      "Iter:   243 | Cost: 0.3792545 | f1: 0.7928994 \n",
      "Iter:   244 | Cost: 0.3803089 | f1: 0.6881029 \n",
      "Iter:   245 | Cost: 0.3798649 | f1: 0.7006369 \n",
      "Iter:   246 | Cost: 0.3825192 | f1: 0.8791209 \n",
      "Iter:   247 | Cost: 0.3926073 | f1: 0.9565217 \n",
      "Iter:   248 | Cost: 0.3858267 | f1: 0.9032258 \n",
      "Iter:   249 | Cost: 0.3801538 | f1: 0.7893175 \n",
      "Iter:   250 | Cost: 0.3838470 | f1: 0.5979381 \n"
     ]
    }
   ],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "\n",
    "wbest = 0\n",
    "bbest = 0\n",
    "abest = 0\n",
    "ccost = 0 \n",
    "for it in range(250):\n",
    "\n",
    "    # weights update by one optimizer step\n",
    "\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "    \n",
    "    '''if accuracy(Y, predictions) > abest:\n",
    "        wbest = weights\n",
    "        bbest = bias\n",
    "        abest = accuracy(Y, predictions)\n",
    "        print('New best')\n",
    "\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "            it + 1, cost(weights, bias, X, Y), acc\n",
    "        )\n",
    "    )'''\n",
    "    prec = metrics.f1_score(Y, predictions, average='binary', pos_label=1)\n",
    "    if  prec > abest or ((prec == abest) and (cost(weights, bias, X, Y) < ccost)):\n",
    "        wbest = weights\n",
    "        bbest = bias\n",
    "        abest = prec\n",
    "        ccost = cost(weights, bias, X, Y)\n",
    "        print(\"New Best:\")\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | f1: {:0.7f} \".format(\n",
    "            it + 1, cost(weights, bias, X, Y), prec\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee2dceeb-5b76-4ec7-9ebb-efb9e562cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yte = np.array(label_test * 2 - np.ones(len(label_test)))\n",
    "Xte = np.array(normalize(sample_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4416ed73-4c7d-44e3-a539-9dba3191327d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.787210742813839, Accuracy: 77.0%\n"
     ]
    }
   ],
   "source": [
    "predictions = [np.sign(variational_classifier(wbest, bbest, x)) for x in Xte]\n",
    "pred = [np.sign(variational_classifier(wbest, bbest, x)) for x in X]\n",
    "acc = accuracy(Yte, predictions)\n",
    "\n",
    "print(f'Cost: {cost(wbest, bbest, Xte, Yte)}, Accuracy: {np.round(acc, 2) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4d2c6851-a0ea-4637-9f25-22a4a0b80c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.77      0.87       200\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.50      0.39      0.44       200\n",
      "weighted avg       1.00      0.77      0.87       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(predictions,Yte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4a2955a-d877-4885-ad84-5f31458a7c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.72      0.79      0.76       141\n",
      "         1.0       0.36      0.27      0.31        59\n",
      "\n",
      "    accuracy                           0.64       200\n",
      "   macro avg       0.54      0.53      0.53       200\n",
      "weighted avg       0.61      0.64      0.62       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(predictions,Yte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dcedeada-a82f-4e82-b738-6a862dc801c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Precision: 0.0%\n",
      "    Recall: 0.0%\n",
      "    f1: 0.0%\n",
      "    Accuracy: 77.0%\n",
      "    Balanced accuracy: 77.0%\n",
      "    Matthew corcorref: 0.0%\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "\n",
    "    Precision: {round(100*metrics.precision_score(predictions,Yte),2)}%\n",
    "    Recall: {round(100*metrics.recall_score(predictions,Yte),2)}%\n",
    "    f1: {round(100*metrics.f1_score(predictions,Yte),2)}%\n",
    "    Accuracy: {round(100*metrics.accuracy_score(predictions,Yte),2)}%\n",
    "    Balanced accuracy: {round(100*metrics.balanced_accuracy_score(predictions,Yte),2)}%\n",
    "    Matthew corcorref: {round(100*metrics.matthews_corrcoef(predictions,Yte),2)}%\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5308be-6630-4637-a387-b4f796aeb98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
