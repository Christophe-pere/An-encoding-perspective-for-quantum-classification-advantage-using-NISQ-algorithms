{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41d3fa6-0ae0-490d-a02b-0dbf81799488",
   "metadata": {},
   "source": [
    "# Quantum benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6011e42-1fab-4f9f-ab99-f4f08607517e",
   "metadata": {},
   "source": [
    "## I - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55142a-b002-440a-833f-8456f6586d22",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA) is similar to PCA, they are linear transformations to reduce the dimensionality of datasets\n",
    "(eigenvalue decomposition). Where PCA will maximize the variance, LDA will maximize the axes for class separation.\n",
    "LDA will create a subspace of k-dimensions from the n-dimensions space of the original data where $k ≤ n − 1$. The\n",
    "subspace is computed taking into account the label to maximize the separation of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5492ce-4f08-4755-b4c2-efd698139101",
   "metadata": {},
   "source": [
    "## II - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90371200-0133-46ab-a205-d0f6b8858e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "#Import classical libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import functools\n",
    "\n",
    "from qiskit import BasicAer\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "from qiskit_machine_learning.kernels import QuantumKernel\n",
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "import logging\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates.embeddings import AngleEmbedding, AmplitudeEmbedding\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "from qiskit.algorithms.optimizers import COBYLA\n",
    "from qiskit.circuit.library import TwoLocal, ZZFeatureMap\n",
    "import qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4552ad2-d5ac-4904-8e45-eafa355e4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598c94c-fdcf-41cd-a4ea-e262932c7e0f",
   "metadata": {},
   "source": [
    "## III - Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e683f3b5-6021-4eb7-bc53-539a5cd79bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out CSV\n",
    "\n",
    "df = pd.read_csv('fraud_detection_bank_dataset.csv', sep=',')\n",
    "df = df.sample(1400)\n",
    "df = df.drop(['Unnamed: 0'], axis = 1) # remove the first column containing the index \n",
    "df_labels = df['targets']\n",
    "df.drop(['targets'],axis = 1,inplace = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Remove highly correlated features \n",
    "cols = ['col_8', 'col_9', 'col_10', 'col_11', 'col_12', 'col_18', 'col_19','col_20', 'col_21', 'col_35', \n",
    "        'col_51', 'col_52', 'col_53', 'col_70','col_71','col_7', 'col_22', 'col_54', 'col_56']\n",
    "\n",
    "X_train = X_train.drop(cols, axis=1)\n",
    "X_test = X_test.drop(cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10255383-ae13-42c1-8d19-b5cf9eca4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69c7f1-b10e-4b54-89ec-89dd5829ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to do an automatic EDA\n",
    "\n",
    "%%script false --no-raise-error\n",
    "\n",
    "import sweetviz as sv\n",
    "\n",
    "#EDA using Autoviz\n",
    "sweet_report = sv.analyze(df)\n",
    "\n",
    "#Saving results to HTML file\n",
    "sweet_report.show_html('sweet_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff48fa-d03b-435d-9343-67186c068b34",
   "metadata": {},
   "source": [
    "## IV - Modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1251e3e-c8ed-4e57-a790-1fa8003f8c31",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4f425-f8b1-40b4-8c3f-4502a9a01a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a948c1c2-cb55-49b2-bb4c-c2d2fdca527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_1 = LDA(n_components=1)\n",
    "lda_2 = LDA(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921a949-e3bd-45b6-b0e0-a84f96788811",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_1.fit(X_train.iloc[:, :46], y_train)\n",
    "feature_1 = lda_1.transform(X_train.iloc[:, :46])\n",
    "lda_2.fit(X_train.iloc[:, 46:], y_train)\n",
    "feature_2 = lda_2.transform(X_train.iloc[:, 46:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f641b-b9b7-498d-9b52-26f2fac93938",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = lda_1.transform(X_test.iloc[:, :46])\n",
    "test_2 = lda_2.transform(X_test.iloc[:, 46:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a6064-c96b-4446-8da3-d160af9592bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_lda_1 = pd.DataFrame(feature_1)\n",
    "features_lda_2 = pd.DataFrame(feature_2)\n",
    "features_lda = features_lda_1.join(features_lda_2, lsuffix=\"_left\", rsuffix=\"_right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32050d4-fbc7-4ab4-ba40-f3a56ae6dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lda_1 =pd.DataFrame(test_1)\n",
    "test_lda_2 =pd.DataFrame(test_2)\n",
    "test_lda = test_lda_1.join(test_lda_2, lsuffix=\"_left\", rsuffix=\"_right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de5099-55d4-4d2d-9ea4-eaf60c27b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = len(features_lda.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd626d5c-ff6a-443a-9700-10a93c24dc74",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf0912-bdbf-4460-ae4c-43016d1f5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ample_train = features_lda.to_numpy()\n",
    "sample_test = test_lda.to_numpy()\n",
    "label_train = y_train\n",
    "label_test = y_test\n",
    "# Normalize\n",
    "\n",
    "std_scale = StandardScaler().fit(sample_train)\n",
    "sample_train = std_scale.transform(sample_train)\n",
    "sample_test = std_scale.transform(sample_test)\n",
    "\n",
    "# Scale for better fit within the feature map\n",
    "\n",
    "minmax_scale = MinMaxScaler((-1, 1)).fit(sample_train)\n",
    "sample_train = minmax_scale.transform(sample_train)\n",
    "sample_test = minmax_scale.transform(sample_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815edbe-c5e7-42d6-9489-39ebb940cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic parameters for hybrid model\n",
    "\n",
    "seed = 8500\n",
    "feature_dim = n_dim\n",
    "num_reps = 2\n",
    "num_shots =256 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17bc9af-4a1c-453d-be99-d54b9e045628",
   "metadata": {},
   "source": [
    "## QSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce7078-9255-4913-b6aa-7f05a0e077c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature_map\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=feature_dim, reps=num_reps)\n",
    "\n",
    "# Define the backend\n",
    "backend = QuantumInstance(\n",
    "    BasicAer.get_backend(\"qasm_simulator\"), shots=num_shots, seed_simulator=seed, seed_transpiler=seed\n",
    ")\n",
    "\n",
    "# Define the kernel\n",
    "\n",
    "kernel = QuantumKernel(feature_map=feature_map, quantum_instance=backend)\n",
    "\n",
    "# Model run\n",
    "svc = SVC(kernel=kernel.evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c063aa2-2125-47a9-adf1-ac7f53444551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from tqdm import tqdm\n",
    "def evaluate_ml_model(_models, X, y, n_fold=10, metric='precision'):\n",
    "    ''' Function to evaluate a ML and QML model with a list of metrics\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    results = pd.DataFrame()\n",
    "    kfold = KFold(n_splits=n_fold)\n",
    "    columns = []\n",
    "    for name, model in tqdm(_models):\n",
    "        # -------------------\n",
    "        # Variables initialization \n",
    "        _df = pd.DataFrame()\n",
    "        names = []\n",
    "        means = []\n",
    "        stds = []\n",
    "        \n",
    "        # -------------------\n",
    "        # k-fold Cross validation\n",
    "        cv_results = cross_validate(model, X, y, cv=kfold, scoring=metric)\n",
    "        \n",
    "        # -------------------\n",
    "        # Compute the mean and standard deviation \n",
    "        for _name, _array in cv_results.items():\n",
    "            names.append(_name)\n",
    "            means.append(round(100*_array.mean(), 2))\n",
    "            stds.append(round(100*_array.std(), 2))\n",
    "        # -------------------\n",
    "        # Save the results in a dataframe \n",
    "        _df =  pd.DataFrame([means, stds], columns=names)\n",
    "        columns.extend([name+' mean (%)', name+' std (%)'])\n",
    "        #results = results.join(_df, on=_df.index)\n",
    "        results = results.append(_df)\n",
    "    results.index = columns\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ebe0d-8690-41ec-af85-590018084cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(max_iter=1000)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('qsvc', svc))\n",
    "_metrics = ['precision', 'recall', 'f1', 'accuracy',  'matthews_corrcoef','balanced_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ce2a3-7a80-42fb-aa8a-80a8da10941a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results = evaluate_ml_model(models, sample_train, label_train, n_fold=10, metric=_metrics)\n",
    "df_results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70b837-4e39-4b65-83ca-8e82fc83efa1",
   "metadata": {},
   "source": [
    "## Pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d5656-f2f2-4d96-b210-a75a161cdb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec49f5-c401-4c36-aae3-57aad3eaa0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angle Encoding\n",
    "\n",
    "num_qubits = n_dim\n",
    "\n",
    "dev = qml.device('default.qubit', wires = num_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(parameters, data):\n",
    "    for i in range(num_qubits):\n",
    "        qml.Hadamard(wires = i)\n",
    "    \n",
    "    AngleEmbedding(features = data, wires = range(num_qubits), rotation = 'Y')\n",
    "    \n",
    "    qml.StronglyEntanglingLayers(weights = parameters, wires = range(num_qubits))\n",
    "    \n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf8fdb-37a0-4b64-b73c-76be553995e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "#print(weights_init, bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49568595-37ec-4749-a230-dc4c6dae2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit(weights_init, sample_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4febb380-0955-4f05-a872-326cc4e3a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a1f84-63e5-4fbb-a647-cba6db4c6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07412a53-c93b-4c73-bd0b-e7e144454435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0b58e-38f3-4c45-a4d2-3d69af787ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff4aa6-c15b-4a59-a718-67d8da2402dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(label_train * 2 - np.ones(len(label_train)),requires_grad=True)  # shift label from {0, 1} to {-1, 1}\n",
    "X = np.array(sample_train, requires_grad=True)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"X = {}, Y = {: d}\".format(list(X[i]), int(Y[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f578a-1984-4814-9070-7b20063c8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = AdamOptimizer(stepsize=0.1, beta1=0.9, beta2=0.99, eps=1e-08)\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22723c-b102-4c87-99c5-62684af992ce",
   "metadata": {},
   "source": [
    "### Train the VQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31d6a7-74f4-4e63-b1cb-b92bfb610a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "\n",
    "wbest = 0\n",
    "bbest = 0\n",
    "abest = 0\n",
    "ccost = 1 \n",
    "for it in range(150):\n",
    "\n",
    "    # weights update by one optimizer step\n",
    "\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "    \n",
    "\n",
    "    prec = metrics.f1_score(Y, predictions, average='binary', pos_label=1)\n",
    "    if  prec > abest or ((prec == abest) and (cost(weights, bias, X, Y) < ccost)):\n",
    "        wbest = weights\n",
    "        bbest = bias\n",
    "        abest = prec\n",
    "        ccost = cost(weights, bias, X, Y)\n",
    "        print('New best')\n",
    "    #prec = metrics.precision_score(Y, predictions, average='binary')\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | f1: {:0.7f} \".format(\n",
    "            it + 1, cost(weights, bias, X, Y), prec\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2dceeb-5b76-4ec7-9ebb-efb9e562cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yte = np.array(label_test * 2 - np.ones(len(label_test)))\n",
    "Xte = np.array(normalize(sample_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416ed73-4c7d-44e3-a539-9dba3191327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [np.sign(variational_classifier(wbest, bbest, x)) for x in Xte]\n",
    "pred = [np.sign(variational_classifier(wbest, bbest, x)) for x in X]\n",
    "acc = accuracy(Yte, predictions)\n",
    "\n",
    "print(f'Cost: {cost(wbest, bbest, Xte, Yte)}, Accuracy: {np.round(acc, 2) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2955a-d877-4885-ad84-5f31458a7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Yte, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcedeada-a82f-4e82-b738-6a862dc801c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "\n",
    "    Precision: {round(100*metrics.precision_score(predictions,Yte),2)}%\n",
    "    Recall: {round(100*metrics.recall_score(predictions,Yte),2)}%\n",
    "    f1: {round(100*metrics.f1_score(predictions,Yte),2)}%\n",
    "    Accuracy: {round(100*metrics.accuracy_score(predictions,Yte),2)}%\n",
    "    Balanced accuracy: {round(100*metrics.balanced_accuracy_score(predictions,Yte),2)}%\n",
    "    Matthew corcorref: {round(100*metrics.matthews_corrcoef(predictions,Yte),2)}%\n",
    "    ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
